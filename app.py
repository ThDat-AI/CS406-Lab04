import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image, ImageOps
import time
import pandas as pd
import altair as alt
import config  # File config ch·ª©a MODEL_PATHS (bao g·ªìm c·∫£ path ResNet), CLASS_NAMES, IMG_SIZE

# --- IMPORT H√ÄM PREPROCESS CHO T·ª™NG LO·∫†I MODEL ---
from tensorflow.keras.applications import mobilenet_v2
from tensorflow.keras.applications import vgg16
from tensorflow.keras.applications import resnet50

# C·∫•u h√¨nh trang
st.set_page_config(page_title="Model Inference Demo", layout="centered")

st.title("üîç Demo Ph√¢n Lo·∫°i ·∫¢nh")

# --- 1. H√ÄM LOAD MODEL (H·ªñ TR·ª¢: MOBILENET, VGG, RESNET) ---
st.sidebar.header("Tr·∫°ng th√°i Model")
loaded_models = {}

@st.cache_resource
def load_model_info(name, path):
    """
    Load model v√† t·ª± ƒë·ªông ch·ªçn ƒë√∫ng h√†m preprocess_input.
    H·ªó tr·ª£: MobileNetV2, VGG16, ResNet50
    """
    # Chu·∫©n h√≥a t√™n v√† ƒë∆∞·ªùng d·∫´n ƒë·ªÉ so s√°nh
    name_lower = name.lower()
    path_lower = path.lower()

    # 1. X√°c ƒë·ªãnh h√†m preprocess d·ª±a tr√™n lo·∫°i model
    if "vgg" in name_lower or "vgg" in path_lower:
        # VGG: Tr·ª´ mean, BGR
        func_preprocess = vgg16.preprocess_input
        # print(f"Log: {name} -> VGG mode")
        
    elif "resnet" in name_lower or "resnet" in path_lower:
        # ResNet: Tr·ª´ mean, BGR (t∆∞∆°ng t·ª± VGG nh∆∞ng kh√°c th√¥ng s·ªë mean)
        func_preprocess = resnet50.preprocess_input
        # print(f"Log: {name} -> ResNet mode")
        
    else:
        # M·∫∑c ƒë·ªãnh l√† MobileNetV2: Scale v·ªÅ [-1, 1]
        func_preprocess = mobilenet_v2.preprocess_input
        # print(f"Log: {name} -> MobileNet mode")
    
    # 2. T·∫°o custom_objects
    # Map nhi·ªÅu key ƒë·ªÉ ƒë·ªÅ ph√≤ng model l∆∞u t√™n h√†m kh√°c nhau
    custom_objects = {
        "preprocess_input": func_preprocess,
        "resnet_preprocess": func_preprocess, # Fix tr∆∞·ªùng h·ª£p l∆∞u t√™n bi·∫øn l√† resnet_preprocess
        "vgg_preprocess": func_preprocess     # Fix tr∆∞·ªùng h·ª£p l∆∞u t√™n bi·∫øn l√† vgg_preprocess
    }
    
    try:
        # Th·ª≠ load v·ªõi custom_objects
        return tf.keras.models.load_model(path, custom_objects=custom_objects)
    except Exception as e1:
        try:
            # Fallback: Th·ª≠ load v·ªõi safe_mode=False
            return tf.keras.models.load_model(path, custom_objects=custom_objects, safe_mode=False)
        except Exception as e2:
            raise e1

# Load to√†n b·ªô model khi app kh·ªüi ƒë·ªông
for name, path in config.MODEL_PATHS.items():
    try:
        model = load_model_info(name, path)
        loaded_models[name] = model
        st.sidebar.success(f"‚úÖ {name} s·∫µn s√†ng")
    except Exception as e:
        st.sidebar.error(f"‚ùå {name} l·ªói: {str(e)}")

# --- 2. H√ÄM X·ª¨ L√ù ·∫¢NH (GI·ªÆ NGUY√äN 0-255) ---
def preprocess_image(image, target_size):
    """
    Chu·∫©n b·ªã ·∫£nh cho model.
    Output: Tensor (1, H, W, 3) v·ªõi gi√° tr·ªã pixel [0, 255]
    L√Ω do: T·∫•t c·∫£ c√°c model (MobileNet, VGG, ResNet) ƒë·ªÅu ƒë√£ c√≥ l·ªõp Lambda 
           b√™n trong ƒë·ªÉ t·ª± x·ª≠ l√Ω (chia ho·∫∑c tr·ª´ mean) t·ª´ input g·ªëc.
    """
    # 1. Chuy·ªÉn sang RGB
    if image.mode != "RGB":
        image = image.convert("RGB")
    
    # 2. Resize ·∫£nh (LANCZOS cho ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t)
    image = ImageOps.fit(image, target_size, Image.Resampling.LANCZOS)
    
    # 3. Chuy·ªÉn sang m·∫£ng numpy
    img_array = np.array(image)
    
    # 4. Th√™m chi·ªÅu batch
    img_array = np.expand_dims(img_array, axis=0)
    
    # 5. Cast v·ªÅ float32 nh∆∞ng KH√îNG chia 255
    img_array = img_array.astype('float32')
    
    return img_array

# --- 3. GIAO DI·ªÜN CH√çNH ---
uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ƒë·ªÉ ph√¢n lo·∫°i (JPG, PNG)", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    # --- HI·ªÇN TH·ªä ·∫¢NH ---
    st.subheader("1. ·∫¢nh ƒë·∫ßu v√†o")
    image = Image.open(uploaded_file)
    
    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        st.image(image, caption='Input Image', use_container_width=True)
    
    st.markdown("---") 

    # Ti·ªÅn x·ª≠ l√Ω
    processed_img = preprocess_image(image, config.IMG_SIZE)

    if not loaded_models:
        st.warning("‚ö†Ô∏è Kh√¥ng c√≥ model n√†o ƒë∆∞·ª£c load th√†nh c√¥ng.")
    else:
        results = []
        
        # ==================================================================
        # B∆Ø·ªöC WARM-UP (L√ÄM N√ìNG ENGINE)
        # ==================================================================
        # Ch·∫°y nh√°p ƒë·ªÉ TensorFlow n·∫°p th∆∞ vi·ªán CUDA v√† kh·ªüi t·∫°o Graph.
        # ƒê·∫£m b·∫£o t√≠nh th·ªùi gian c√¥ng b·∫±ng cho t·∫•t c·∫£ model.
        with st.spinner("ƒêang kh·ªüi ƒë·ªông engine & Warm-up models..."):
            for _, model in loaded_models.items():
                model.predict(processed_img, verbose=0)
        
        # ==================================================================
        # B·∫ÆT ƒê·∫¶U ƒêO TH·ªúI GIAN TH·ª∞C
        # ==================================================================
        for model_name, model in loaded_models.items():
            # D√πng perf_counter cho ƒë·ªô ch√≠nh x√°c cao (micro-second)
            start_time = time.perf_counter()
            
            predictions = model.predict(processed_img, verbose=0)
            
            end_time = time.perf_counter()
            inf_time = end_time - start_time
            
            # X·ª≠ l√Ω output
            if predictions.shape[1] > 1:
                idx = np.argmax(predictions[0])
                conf = np.max(predictions[0]) * 100
                label = config.CLASS_NAMES[idx] if idx < len(config.CLASS_NAMES) else f"Class {idx}"
            else:
                score = predictions[0][0]
                conf = score * 100 if score > 0.5 else (1 - score) * 100
                label = config.CLASS_NAMES[1] if score > 0.5 else config.CLASS_NAMES[0]
            
            results.append({
                "T√™n Model": model_name,
                "D·ª± ƒëo√°n": label,
                "ƒê·ªô tin c·∫≠y (%)": round(conf, 2),
                "Th·ªùi gian (s)": round(inf_time, 4)
            })
        
        # T·∫°o DataFrame
        df = pd.DataFrame(results)
        df.insert(0, 'STT', range(1, 1 + len(df)))

        # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        st.subheader("2. K·∫øt qu·∫£ chi ti·∫øt")
        st.dataframe(df, use_container_width=True, hide_index=True)
        
        st.markdown("---")

        # Logic t√¥ m√†u bi·ªÉu ƒë·ªì (Highlight Best Performance)
        if not df.empty:
            best_conf = df["ƒê·ªô tin c·∫≠y (%)"].max()
            best_time = df["Th·ªùi gian (s)"].min()

            df['color_conf'] = df['ƒê·ªô tin c·∫≠y (%)'].apply(
                lambda x: config.CHART_COLOR_HIGHLIGHT if x == best_conf else config.CHART_COLOR_NORMAL
            )
            df['color_time'] = df['Th·ªùi gian (s)'].apply(
                lambda x: config.CHART_COLOR_HIGHLIGHT if x == best_time else config.CHART_COLOR_NORMAL
            )

            # Bi·ªÉu ƒë·ªì Confidence
            st.subheader("3. So s√°nh ƒë·ªô tin c·∫≠y (Confidence)")
            chart_conf = alt.Chart(df).mark_bar().encode(
                x=alt.X('T√™n Model', axis=alt.Axis(labelAngle=0)),
                y='ƒê·ªô tin c·∫≠y (%)',
                color=alt.Color('color_conf', scale=None),
                tooltip=['T√™n Model', 'D·ª± ƒëo√°n', 'ƒê·ªô tin c·∫≠y (%)']
            ).properties(height=300)
            st.altair_chart(chart_conf, use_container_width=True)

            st.markdown("---")

            # Bi·ªÉu ƒë·ªì Inference Time
            st.subheader("4. So s√°nh t·ªëc ƒë·ªô (Inference Time)")
            chart_time = alt.Chart(df).mark_bar().encode(
                x=alt.X('T√™n Model', axis=alt.Axis(labelAngle=0)),
                y='Th·ªùi gian (s)',
                color=alt.Color('color_time', scale=None),
                tooltip=['T√™n Model', 'Th·ªùi gian (s)']
            ).properties(height=300)
            st.altair_chart(chart_time, use_container_width=True)